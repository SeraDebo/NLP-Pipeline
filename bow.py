# -*- coding: utf-8 -*-
"""nlp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Al0Vs_xbusvqjgNMxkpmC0i8uOphAoBf
"""

import nltk

nltk.download('punkt_tab')

from nltk.tokenize import sent_tokenize, word_tokenize

corpus = """
I love to play cricket! I often go to park. I love to eat ice cream. I have a pen and a book
"""
sentence_tokens = sent_tokenize(corpus)
word_tokens = word_tokenize(corpus)

print("Sent_Tokens:", sentence_tokens)
print("Word_Tokens:", word_tokens)

import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

df = pd.read_csv('./df_ar.csv')

print(df['text'].head())

vectorizer = CountVectorizer(max_features=1000)

X = vectorizer.fit_transform(df['text'].astype(str))

bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())

print(bow_df.head())

#---- N-Grams -----

print("\n--- Unigrams ---")
vectorizer_uni = CountVectorizer(ngram_range=(1, 2))
X_uni = vectorizer_uni.fit_transform(df['text'])
df_uni = pd.DataFrame(X_uni[:100].toarray(), columns=vectorizer_uni.get_feature_names_out())
print(df_uni.head())

# ---------- BIGRAMS ----------
print("\n--- Bigrams ---")
vectorizer_bi = CountVectorizer(ngram_range=(2, 2))
X_bi = vectorizer_bi.fit_transform(df['text'])
df_bi = pd.DataFrame(X_bi[:100].toarray(), columns=vectorizer_bi.get_feature_names_out())
print(df_bi.head())

# ---------- TRIGRAMS ----------
print("\n--- Trigrams ---")
vectorizer_tri = CountVectorizer(ngram_range=(3, 3))
X_tri = vectorizer_tri.fit_transform(df['text'])
df_tri = pd.DataFrame(X_tri[:100].toarray(), columns=vectorizer_tri.get_feature_names_out())
print(df_tri.head())

